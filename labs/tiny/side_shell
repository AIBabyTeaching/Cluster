python - <<'PY'
import os, datasets, transformers, json
os.environ["HF_HOME"] = '/home/yhanafy/.cache/huggingface/hf_tiny'
datasets.load_dataset("ag_news", split="train[:2000]",
                      cache_dir=f"{os.environ['HF_HOME']}/ds")
tok = transformers.AutoTokenizer.from_pretrained(
        "google/bert_uncased_L-2_H-128_A-2",
        cache_dir=f"{os.environ['HF_HOME']}/tok")
print("Done pre-caching.")
PY


salloc -N1 -n1 -c2 -p parallel --time=00:05:00 --exclusive

# 2) env + rendez-vous
source ~/llamaenv_local/bin/activate
cd ~/mrmito/project
export HF_HOME=/home/yhanafy/.cache/hugging_face/hf_tiny
export PYTHONPATH=$PWD:$PYTHONPATH
export MASTER_ADDR=$(hostname)
export MASTER_PORT=$((20000 + RANDOM % 10000))

torchrun \
  --nproc_per_node=2 \
  --rdzv_backend=c10d \
  --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
  labs/tiny/train_tiny.py \
  --subset 2000 --epochs 3
  
_________________________________________________


export HF_HOME=/home/yhanafy/.cache/huggingface/hf_tiny
python labs/tiny/test_tiny.py --ckpt tiny_out