# ------------ 0) Pre-cache (once) ------------------------
python - <<'PY'
import os, datasets, transformers
os.environ["HF_HOME"] = os.path.expanduser("~/.cache/hf_rag")
datasets.load_dataset("ag_news", split="train[:2000]",
                      cache_dir=f"{os.environ['HF_HOME']}/ds")
transformers.AutoTokenizer.from_pretrained(
    "google/flan-t5-small", cache_dir=f"{os.environ['HF_HOME']}/tok")
transformers.AutoModelForSeq2SeqLM.from_pretrained(
    "google/flan-t5-small", cache_dir=f"{os.environ['HF_HOME']}/gen")
print("Done pre-caching for RAG.")
PY

# ------------ 1) Allocation + env ------------------------
salloc --partition=torch --nodes=20 --ntasks-per-node=1 --cpus-per-task=4 --time=00:30:00

# inside the allocated shell:
source ~/llamaenv_local/bin/activate
cd ~/mrmito/project
export HF_HOME=$HOME/.cache/hf_rag
export PYTHONPATH=$PWD:$PYTHONPATH

# ------------ 2) Single-query demo -----------------------
python labs/ragging/rag_example.py --dry_run --query "What is deep learning?"

# ------------ 3) Parallel batch demo ---------------------
# (create a small queries file if you don't have it)
cat > labs/ragging/queries.txt <<'EOF'
What happened in the sports world?
Which company announced a new product?
How did the stock market perform?
Describe a political event mentioned.
EOF

# Optional explicit rendezvous (single-node)
export MASTER_ADDR=$(hostname)
export MASTER_PORT=$((20000 + RANDOM % 10000))

torchrun \
  --nproc_per_node=2 \
  labs/ragging/rag_example.py \
  --dry_run --queries_file labs/ragging/queries.txt --k 3 --max_new_tokens 64

# ------------ 4) Monitoring (in another terminal) --------
watch -n2 "sstat -j $SLURM_JOB_ID --format=JobID,MaxRSS,AveCPU,MaxVMSize"
watch -n2 "ps -u $USER -o pid,pcpu,pmem,etime,cmd | grep python | grep -v grep"
watch -n5 'squeue -u $USER -o "%.9i %.2t %.10M %.18R %.12C %j"'
